1. 工作流程
(1) 使用 MTCNN 检测人脸并提取关键点
输入：原始图片或视频帧。
过程：
用 MTCNN 检测图片中的所有人脸。
获取人脸边界框（Bounding Box）和关键点（眼睛、鼻子、嘴角位置）。
输出：
每个人脸的边界框坐标。
关键点坐标，用于对齐人脸。
(2) 对人脸进行对齐
输入：MTCNN 提取的关键点坐标。
过程：
根据人脸关键点（如两眼中心）计算仿射变换矩阵。
对图像进行几何变换，将人脸调整为固定的姿态（通常是正面）。
输出：
对齐后的人脸图像，消除角度偏差。
(3) 使用 FaceNet 提取人脸特征
输入：对齐后的人脸图像。
过程：
将对齐后的图像传入 FaceNet 模型。
FaceNet 输出固定长度的特征向量（128维），表示该人脸的深度特征。
输出：
每个人脸的特征向量。
(4) 计算人脸相似性或分类
输入：人脸特征向量。
过程：
验证：计算两个人脸特征向量的欧几里得距离或余弦相似度，判断是否为同一人。
识别：将特征向量与已知人脸库中的向量进行比较，找到最匹配的身份。
输出：
人脸相似度分数或识别结果。



from mtcnn import MTCNN
from facenet_pytorch import InceptionResnetV1
from PIL import Image
import torch
import numpy as np

加载 MTCNN 和 FaceNet 模型
mtcnn = MTCNN(keep_all=True)  # 检测所有人脸
facenet = InceptionResnetV1(pretrained='vggface2').eval()  # 使用预训练的 FaceNet 模型

读取图像
image_path = "input_image.jpg"
image = Image.open(image_path)

步骤 1: 使用 MTCNN 检测人脸
boxes, probs, landmarks = mtcnn.detect(image, landmarks=True)

if boxes is not None:
    aligned_faces = []
    for i, box in enumerate(boxes):
        步骤 2: 根据关键点对齐人脸
        face = mtcnn.extract(image, box, save_path=None)  # 提取对齐人脸
        aligned_faces.append(face)
    
    步骤 3: 使用 FaceNet 提取人脸特征
    embeddings = []
    for face in aligned_faces:
        face_tensor = torch.unsqueeze(torch.tensor(np.array(face)).permute(2, 0, 1), 0)  # 转换为 FaceNet 输入
        embedding = facenet(face_tensor)
        embeddings.append(embedding.detach().numpy())
    
    步骤 4: 比较特征向量 (示例：两张人脸的相似性计算)
    if len(embeddings) >= 2:
        distance = np.linalg.norm(embeddings[0] - embeddings[1])
        print(f"人脸特征向量之间的距离: {distance}")
else:
    print("未检测到人脸")

